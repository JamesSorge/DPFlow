{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458dd7c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding */DPFlow/architecture_files to sys.path\n",
      "sys.path = \n",
      "\t/opt/conda/lib/python311.zip\n",
      "\t/opt/conda/lib/python3.11\n",
      "\t/opt/conda/lib/python3.11/lib-dynload\n",
      "\t\n",
      "\t/opt/conda/lib/python3.11/site-packages\n",
      "\t/home/jsorge/private/final_project/DPFlow\n",
      "Using cuda device.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "path_list = sys.path\n",
    "found_directories = False\n",
    "for path_variable in path_list:\n",
    "    if \"DPFlow\" in path_variable:\n",
    "        found_directories = True\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "if not found_directories:\n",
    "    print(f\"Adding */DPFlow/architecture_files to sys.path\")\n",
    "    sys.path.append(\"/home/jsorge/private/final_project/DPFlow\")\n",
    "    # sys.path.append(\"/home/yix050/private/ECE285_Visual_25Spring/final_project/DPFlow\")\n",
    "    print(f\"sys.path = \")\n",
    "    for idx in range(len(sys.path)):\n",
    "        print(f\"\\t{sys.path[idx]}\") \n",
    "from architecture_files.cross_gated_unit import CrossGateBlock, DownsamplingLayer\n",
    "from architecture_files.res_stem import ResStem\n",
    "from architecture_files.conv_gru_cell import ConvGRUCell\n",
    "\n",
    "# Set device to CUDA\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2149417",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class BidirEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The hidden_channels is always [64, 96, 128] in the DPFlow paper, and pyramid_level = 3\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_channels=[64, 96, 128]):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Conv Stem: takes the raw image as input and outputs X0 and H0 ###\n",
    "        self.conv_stem = ResStem([hidden_channels[0], hidden_channels[1], 2 * hidden_channels[2]])\n",
    "        self.lower_stem = ResStem([hidden_channels[0], hidden_channels[1], hidden_channels[2]])\n",
    "\n",
    "        ### Forward GRU ###\n",
    "        self.forward_gru = ConvGRUCell(hidden_channels[-1], hidden_channels[-1])\n",
    "        # for passing Hf to the lower scale level. H_out = H_in/2\n",
    "        self.down_gru = nn.Conv2d(hidden_channels[-1], hidden_channels[-1], kernel_size=3, stride=2, padding=1, bias=True) \n",
    "\n",
    "        ### Backward GRU ###\n",
    "        self.backward_gru = ConvGRUCell(hidden_channels[-1], hidden_channels[-1])\n",
    "        # for passing Hb and Xb to the higher scale level. H_out = 2*H_in\n",
    "        self.up_gru = nn.ConvTranspose2d(hidden_channels[-1], hidden_channels[-1], kernel_size=4, stride=2, padding=1, bias=True) \n",
    "\n",
    "        ### Forward CGU ###\n",
    "        self.downsampling = DownsamplingLayer(channels=hidden_channels[-1])\n",
    "        self.forward_cgu = CrossGateBlock(num_channels_in=hidden_channels[-1], \n",
    "                                          num_channels_hidden=hidden_channels[-1], \n",
    "                                          norm_type=\"batch_norm\", \n",
    "                                          use_dropout=True, \n",
    "                                          use_layer_scale=False)\n",
    "        \n",
    "        ### Backward CGU ###\n",
    "        self.backward_cgu = CrossGateBlock(num_channels_in=hidden_channels[-1], \n",
    "                                          num_channels_hidden=hidden_channels[-1], \n",
    "                                          norm_type=\"batch_norm\", \n",
    "                                          use_dropout=True, \n",
    "                                          use_layer_scale=False)\n",
    "\n",
    "        ### Output layer ###\n",
    "        self.num_out_stages = 1 # DPFlow always sets this to 1\n",
    "        self.out_1x1_abs_chs = 384 # DPFlow always sets this to 384\n",
    "        self.out_1x1_factor = None\n",
    "\n",
    "        if self.num_out_stages > 0:\n",
    "            # This out_merge_conv has a ReLU layer before it\n",
    "            self.out_merge_conv = nn.Conv2d(3 * hidden_channels[-1], hidden_channels[-1], kernel_size=1)\n",
    "            self.out_cgu = CrossGateBlock(num_channels_in=hidden_channels[-1], \n",
    "                                          num_channels_hidden=hidden_channels[-1], \n",
    "                                          norm_type=\"batch_norm\", \n",
    "                                          use_dropout=True, \n",
    "                                          use_layer_scale=False)\n",
    "        if self.out_1x1_abs_chs > 0:\n",
    "            self.out_1x1 = nn.Conv2d(hidden_channels[-1], self.out_1x1_abs_chs, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.tensor, y: torch.tensor, pyr_levels: int):\n",
    "        \"\"\"\n",
    "        Takes two frames of image x and y as input. x and y will go through the same process separately.\n",
    "        @param x: a raw image\n",
    "        @param y: a raw image\n",
    "\n",
    "        @return: Two feature pyramids x_pyramid[::-1], y_pyramid[::-1] as the embeddings to pass to the decode\n",
    "        \"\"\"\n",
    "\n",
    "        # input_x = x # for concatenation in the final feature pyramid\n",
    "        # input_y = y # for concatenation in the final feature pyramid\n",
    "        \n",
    "        x0, hx0 = self._get_init_stat(x)\n",
    "        y0, hy0 = self._get_init_stat(y)\n",
    "        \n",
    "        x_pyramid, y_pyramid = self._encode(x0, hx0, y0, hy0, pyr_levels, x, y)\n",
    "        \n",
    "        return x_pyramid[::-1], y_pyramid[::-1]\n",
    "\n",
    "    \n",
    "    def _encode(self, x0, hx0, y0, hy0, pyr_levels, input_x, input_y):\n",
    "        #TODO: Implement the dual-pyramid encoder block\n",
    "        \"\"\"\n",
    "        Go through the forward process (high scale to low scale) and then the backward process (low scale to high scale)\n",
    "        Returns the feature pyramids x_pyramid and y_pyramid\n",
    "        \"\"\"\n",
    "        \n",
    "        x_pyramid = [None] * 3 # store concatenation of xf, xb, xi for each scale\n",
    "        y_pyramid = [None] * 3 # store concatenation of yf, yb, yi for each scale\n",
    "        \n",
    "        # input_x = x0 # for concatenation in the final feature pyramid\n",
    "        # input_y = y0 # for concatenation in the final feature pyramid\n",
    "        \n",
    "        ####### Forward Start #######\n",
    "        x_forwards = []\n",
    "        y_forwards = []\n",
    "\n",
    "        x_f, hx_f = x0, hx0\n",
    "        y_f, hy_f = y0, hy0\n",
    "        \n",
    "        for i in range(pyr_levels):\n",
    "            hx_f = self.forward_gru(x_f, hx_f)\n",
    "            hy_f = self.forward_gru(y_f, hy_f)\n",
    "\n",
    "            x_f, y_f = self.forward_cgu(hx_f, hy_f) # this is used as xf for the next scale, and in the final concatenation\n",
    "            x_f = self.downsampling(x_f)\n",
    "            y_f = self.downsampling(y_f)\n",
    "            x_f = x_f.contiguous() # make the data contiguous to speed up the computation?\n",
    "            y_f = y_f.contiguous()\n",
    "\n",
    "            # downsample the forward h\n",
    "            if (i < pyr_levels - 1): # don't do the down_gru for the lowest level\n",
    "                hx_f = torch.tanh(self.down_gru(hx_f))\n",
    "                hy_f = torch.tanh(self.down_gru(hy_f))\n",
    "\n",
    "            x_forwards.append(x_f)\n",
    "            y_forwards.append(y_f)\n",
    "        ####### Forward End #######\n",
    "        \n",
    "\n",
    "        ####### Backward Start #######\n",
    "        # Initialize the hx and hy when doing backward from the lowest scale layer\n",
    "        hx_b = torch.zeros_like(x_forwards[-1])\n",
    "        hy_b = torch.zeros_like(y_forwards[-1])\n",
    "\n",
    "        for i in range(len(x_forwards) - 1, -1, -1):\n",
    "            x_f = x_forwards[i]\n",
    "            y_f = y_forwards[i]\n",
    "\n",
    "            hx_b = self.backward_gru(x_f, hx_b)\n",
    "            hy_b = self.backward_gru(y_f, hy_b)\n",
    "\n",
    "            x_b, y_b = self.backward_cgu(hx_b, hy_b)\n",
    "\n",
    "            # Downsample the input for concatenating with different scales, (1/2, 1/4, 1/8) for each scale.\n",
    "            input_x_lower = F.interpolate(input_x, scale_factor=(1 / 2**(i+1)), mode='bilinear', align_corners=True)\n",
    "            input_y_lower = F.interpolate(input_y, scale_factor=(1 / 2**(i+1)), mode='bilinear', align_corners=True)\n",
    "            \n",
    "            input_x_lower = self.lower_stem(input_x_lower)\n",
    "            input_y_lower = self.lower_stem(input_y_lower)\n",
    "\n",
    "            # These pyramids are the output of this Encoder\n",
    "            # Sizes of tensors must match except in dimension 1 (height and width must match, will concate along the channel).\n",
    "            x_pyramid[i] = torch.cat([x_f, x_b, input_x_lower], 1)\n",
    "            y_pyramid[i] = torch.cat([y_f, y_b, input_y_lower], 1) \n",
    "            \n",
    "            # upsample the backward h\n",
    "            if i > 0:\n",
    "                hx_b = torch.tanh(self.up_gru(hx_b))\n",
    "                hy_b = torch.tanh(self.up_gru(hy_b))\n",
    "        ####### Backward End #######\n",
    "\n",
    "        ####### Output Layers ######\n",
    "        for i, (x, y) in enumerate(zip(x_pyramid, y_pyramid)):\n",
    "            if self.num_out_stages > 0:\n",
    "                x = self.out_merge_conv(F.relu(x))\n",
    "                y = self.out_merge_conv(F.relu(y))\n",
    "                x, y = self.out_cgu(x, y)\n",
    "            if self.out_1x1_abs_chs > 0:\n",
    "                if self.out_1x1_factor is None:\n",
    "                    x = self.out_1x1(x)\n",
    "                    y = self.out_1x1(y)\n",
    "                else:\n",
    "                    x = self.out_1x1(x, int(self.out_1x1_factor * x.shape[1]))\n",
    "                    y = self.out_1x1(y, int(self.out_1x1_factor * y.shape[1]))\n",
    "            x_pyramid[i] = x\n",
    "            y_pyramid[i] = y\n",
    "        \n",
    "        \n",
    "        return x_pyramid, y_pyramid\n",
    "\n",
    "\n",
    "    def _get_init_stat(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input image x to the conv_stem, and return x0, h0\n",
    "        \"\"\"\n",
    "        x = self.conv_stem(x)\n",
    "        x, hx = torch.split(x, [x.shape[1] // 2, x.shape[1] // 2], 1)\n",
    "        hx = torch.tanh(hx)\n",
    "        return x, hx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149660fa-baf9-4994-b351-dab1639d000e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (63) must match the size of tensor b (64) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m BidirEncoder()\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_img1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_img2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_pyr_levels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 71\u001b[0m, in \u001b[0;36mBidirEncoder.forward\u001b[0;34m(self, x, y, pyr_levels)\u001b[0m\n\u001b[1;32m     68\u001b[0m x0, hx0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_init_stat(x)\n\u001b[1;32m     69\u001b[0m y0, hy0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_init_stat(y)\n\u001b[0;32m---> 71\u001b[0m x_pyramid, y_pyramid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhy0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpyr_levels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_pyramid[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], y_pyramid[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 125\u001b[0m, in \u001b[0;36mBidirEncoder._encode\u001b[0;34m(self, x0, hx0, y0, hy0, pyr_levels, input_x, input_y)\u001b[0m\n\u001b[1;32m    122\u001b[0m x_f \u001b[38;5;241m=\u001b[39m x_forwards[i]\n\u001b[1;32m    123\u001b[0m y_f \u001b[38;5;241m=\u001b[39m y_forwards[i]\n\u001b[0;32m--> 125\u001b[0m hx_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_gru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m hy_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_gru(y_f, hy_b)\n\u001b[1;32m    128\u001b[0m x_b, y_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_cgu(hx_b, hy_b)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/private/final_project/DPFlow/architecture_files/conv_gru_cell.py:28\u001b[0m, in \u001b[0;36mConvGRUCell.forward\u001b[0;34m(self, x, h_prev)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mtensor, h_prev: torch\u001b[38;5;241m.\u001b[39mtensor):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    @rtype:   torch.tensor\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    @return:  hidden representation of this ConvGRU cell with shape (batch, channel, height, width)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_Wz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_Uz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_prev\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_Wr(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_Ur(h_prev))\n\u001b[1;32m     30\u001b[0m     h_hat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_W(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_U(r\u001b[38;5;241m*\u001b[39mh_prev))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (63) must match the size of tensor b (64) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "example_img1 = torch.rand((2, 3, 1000, 1000), device=device)\n",
    "example_img2 = torch.rand((2, 3, 1000, 1000), device=device)\n",
    "example_pyr_levels = 3\n",
    "\n",
    "model = BidirEncoder()\n",
    "model.to(device)\n",
    "output = model(example_img1, example_img2, example_pyr_levels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

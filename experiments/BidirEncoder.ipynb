{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458dd7c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding */DPFlow/architecture_files to sys.path\n",
      "sys.path = \n",
      "\t/opt/conda/lib/python311.zip\n",
      "\t/opt/conda/lib/python3.11\n",
      "\t/opt/conda/lib/python3.11/lib-dynload\n",
      "\t\n",
      "\t/opt/conda/lib/python3.11/site-packages\n",
      "\t/home/jsorge/private/final_project/DPFlow\n",
      "Using cuda device.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "path_list = sys.path\n",
    "found_directories = False\n",
    "for path_variable in path_list:\n",
    "    if \"DPFlow\" in path_variable:\n",
    "        found_directories = True\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "if not found_directories:\n",
    "    print(f\"Adding */DPFlow/architecture_files to sys.path\")\n",
    "    sys.path.append(\"/home/jsorge/private/final_project/DPFlow\")\n",
    "    # sys.path.append(\"/home/yix050/private/ECE285_Visual_25Spring/final_project/DPFlow\")\n",
    "    print(f\"sys.path = \")\n",
    "    for idx in range(len(sys.path)):\n",
    "        print(f\"\\t{sys.path[idx]}\") \n",
    "from architecture_files.cross_gated_unit import CrossGateBlock, DownsamplingLayer\n",
    "from architecture_files.res_stem import ResStem\n",
    "from architecture_files.conv_gru_cell import ConvGRUCell\n",
    "\n",
    "# Set device to CUDA\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2149417",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class BidirEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The hidden_channels is always [64, 96, 128] in the DPFlow paper, and pyramid_level = 3\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_channels=[64, 96, 128]):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Conv Stem: takes the raw image as input and outputs X0 and H0 ###\n",
    "        self.conv_stem = ResStem([hidden_channels[0], hidden_channels[1], 2 * hidden_channels[2]])\n",
    "        self.lower_stem = ResStem([hidden_channels[0], hidden_channels[1], hidden_channels[2]])\n",
    "\n",
    "        ### Forward GRU ###\n",
    "        self.forward_gru = ConvGRUCell(hidden_channels[-1], hidden_channels[-1])\n",
    "        # for passing Hf to the lower scale level. H_out = H_in/2\n",
    "        self.down_gru = nn.Conv2d(hidden_channels[-1], hidden_channels[-1], kernel_size=3, stride=2, padding=1, bias=True) \n",
    "\n",
    "        ### Backward GRU ###\n",
    "        self.backward_gru = ConvGRUCell(hidden_channels[-1], hidden_channels[-1])\n",
    "        # for passing Hb and Xb to the higher scale level. H_out = 2*H_in\n",
    "        self.up_gru = nn.ConvTranspose2d(hidden_channels[-1], hidden_channels[-1], kernel_size=4, stride=2, padding=1, bias=True) \n",
    "\n",
    "        ### Forward CGU ###\n",
    "        self.downsampling = DownsamplingLayer(channels=hidden_channels[-1])\n",
    "        self.forward_cgu = CrossGateBlock(num_channels_in=hidden_channels[-1], \n",
    "                                          num_channels_hidden=hidden_channels[-1], \n",
    "                                          norm_type=\"batch_norm\", \n",
    "                                          use_dropout=True, \n",
    "                                          use_layer_scale=False)\n",
    "        \n",
    "        ### Backward CGU ###\n",
    "        self.backward_cgu = CrossGateBlock(num_channels_in=hidden_channels[-1], \n",
    "                                          num_channels_hidden=hidden_channels[-1], \n",
    "                                          norm_type=\"batch_norm\", \n",
    "                                          use_dropout=True, \n",
    "                                          use_layer_scale=False)\n",
    "\n",
    "        ### Output layer ###\n",
    "        self.num_out_stages = 1 # DPFlow always sets this to 1\n",
    "        self.out_1x1_abs_chs = 384 # DPFlow always sets this to 384\n",
    "\n",
    "        if self.num_out_stages > 0:\n",
    "            # This out_merge_conv has a ReLU layer before it\n",
    "            self.out_merge_conv = nn.Conv2d(3 * hidden_channels[-1], hidden_channels[-1], kernel_size=1)\n",
    "            self.out_cgu = CrossGateBlock(num_channels_in=self.num_out_stages, \n",
    "                                          num_channels_hidden=hidden_channels[-1], \n",
    "                                          norm_type=\"batch_norm\", \n",
    "                                          use_dropout=True, \n",
    "                                          use_layer_scale=False)\n",
    "        if self.out_1x1_abs_chs > 0:\n",
    "            self.out_1x1 = nn.Conv2d(hidden_channels[-1], self.out_1x1_abs_chs, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.tensor, y: torch.tensor, pyr_levels: int):\n",
    "        \"\"\"\n",
    "        Takes two frames of image x and y as input. x and y will go through the same process separately.\n",
    "        @param x: a raw image\n",
    "        @param y: a raw image\n",
    "\n",
    "        @return: Two feature pyramids x_pyramid[::-1], y_pyramid[::-1] as the embeddings to pass to the decode\n",
    "        \"\"\"\n",
    "\n",
    "        # input_x = x # for concatenation in the final feature pyramid\n",
    "        # input_y = y # for concatenation in the final feature pyramid\n",
    "        \n",
    "        x0, hx0 = self._get_init_stat(x)\n",
    "        y0, hy0 = self._get_init_stat(y)\n",
    "        \n",
    "        x_pyramid, y_pyramid = self._encode(x0, hx0, y0, hy0, pyr_levels, x, y)\n",
    "        \n",
    "        return x_pyramid[::-1], y_pyramid[::-1]\n",
    "\n",
    "    \n",
    "    def _encode(self, x0, hx0, y0, hy0, pyr_levels, input_x, input_y):\n",
    "        #TODO: Implement the dual-pyramid encoder block\n",
    "        \"\"\"\n",
    "        Go through the forward process (high scale to low scale) and then the backward process (low scale to high scale)\n",
    "        Returns the feature pyramids x_pyramid and y_pyramid\n",
    "        \"\"\"\n",
    "        \n",
    "        x_pyramid = [None] * 3 # store concatenation of xf, xb, xi for each scale\n",
    "        y_pyramid = [None] * 3 # store concatenation of yf, yb, yi for each scale\n",
    "        \n",
    "        # input_x = x0 # for concatenation in the final feature pyramid\n",
    "        # input_y = y0 # for concatenation in the final feature pyramid\n",
    "        \n",
    "        ####### Forward Start #######\n",
    "        x_forwards = []\n",
    "        y_forwards = []\n",
    "\n",
    "        x_f, hx_f = x0, hx0\n",
    "        y_f, hy_f = y0, hy0\n",
    "        \n",
    "        for i in range(pyr_levels):\n",
    "            print(f\"x_f shape: {x_f.shape}\")\n",
    "            print(f\"h_f shape: {hx_f.shape}\")\n",
    "            hx_f = self.forward_gru(x_f, hx_f)\n",
    "            hy_f = self.forward_gru(y_f, hy_f)\n",
    "\n",
    "            x_f, y_f = self.forward_cgu(hx_f, hy_f) # this is used as xf for the next scale, and in the final concatenation\n",
    "            x_f = self.downsampling(x_f)\n",
    "            y_f = self.downsampling(y_f)\n",
    "            x_f = x_f.contiguous() # make the data contiguous to speed up the computation?\n",
    "            y_f = y_f.contiguous()\n",
    "\n",
    "            # downsample the forward h\n",
    "            if (i < pyr_levels - 1): # don't do the down_gru for the lowest level\n",
    "                hx_f = torch.tanh(self.down_gru(hx_f))\n",
    "                hy_f = torch.tanh(self.down_gru(hy_f))\n",
    "\n",
    "            x_forwards.append(x_f)\n",
    "            y_forwards.append(y_f)\n",
    "        ####### Forward End #######\n",
    "        \n",
    "\n",
    "        ####### Backward Start #######\n",
    "        # Initialize the hx and hy when doing backward from the lowest scale layer\n",
    "        hx_b = torch.zeros_like(x_forwards[-1])\n",
    "        hy_b = torch.zeros_like(y_forwards[-1])\n",
    "\n",
    "        print(f\"len x_forwards: {len(x_forwards)}\")\n",
    "        for i in range(len(x_forwards) - 1, -1, -1):\n",
    "            x_f = x_forwards[i]\n",
    "            y_f = y_forwards[i]\n",
    "\n",
    "            hx_b = self.backward_gru(x_f, hx_b)\n",
    "            hy_b = self.backward_gru(y_f, hy_b)\n",
    "\n",
    "            x_b, y_b = self.backward_cgu(hx_b, hy_b)\n",
    "\n",
    "            # Downsample the input for concatenating with different scales, (1/2, 1/4, 1/8) for each scale.\n",
    "            print(f\"input_x before interpolate: {input_x.shape}\")\n",
    "            input_x_lower = F.interpolate(input_x, scale_factor=(1 / 2**(i+1)), mode='bilinear', align_corners=True)\n",
    "            input_y_lower = F.interpolate(input_y, scale_factor=(1 / 2**(i+1)), mode='bilinear', align_corners=True)\n",
    "            print(f\"input_x after interpolate: {input_x_lower.shape}\")\n",
    "            \n",
    "            input_x_lower = self.lower_stem(input_x_lower)\n",
    "            input_y_lower = self.lower_stem(input_y_lower)\n",
    "\n",
    "            # These pyramids are the output of this Encoder\n",
    "            # Sizes of tensors must match except in dimension 1 (height and width must match, will concate along the channel).\n",
    "            x_pyramid[i] = torch.cat([x_f, x_b, input_x_lower], 1)\n",
    "            y_pyramid[i] = torch.cat([y_f, y_b, input_y_lower], 1) \n",
    "            \n",
    "            # upsample the backward h\n",
    "            if i > 0:\n",
    "                hx_b = torch.tanh(self.up_gru(hx_b))\n",
    "                hy_b = torch.tanh(self.up_gru(hy_b))\n",
    "        ####### Backward End #######\n",
    "\n",
    "        ####### Output Layers ######\n",
    "        for i, (x, y) in enumerate(zip(x_pyramid, y_pyramid)):\n",
    "            if self.num_out_stages > 0:\n",
    "                x = self.out_merge_conv(F.relu(x))\n",
    "                y = self.out_merge_conv(F.relu(y))\n",
    "                x, y = self.out_cgu(x, y)\n",
    "            if self.out_1x1_abs_chs > 0:\n",
    "                if self.out_1x1_factor is None:\n",
    "                    x = self.out_1x1(x)\n",
    "                    y = self.out_1x1(y)\n",
    "                else:\n",
    "                    x = self.out_1x1(x, int(self.out_1x1_factor * x.shape[1]))\n",
    "                    y = self.out_1x1(y, int(self.out_1x1_factor * y.shape[1]))\n",
    "            x_pyramid[i] = x\n",
    "            y_pyramid[i] = y\n",
    "        \n",
    "        \n",
    "        return x_pyramid, y_pyramid\n",
    "\n",
    "\n",
    "    def _get_init_stat(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input image x to the conv_stem, and return x0, h0\n",
    "        \"\"\"\n",
    "        x = self.conv_stem(x)\n",
    "        x, hx = torch.split(x, [x.shape[1] // 2, x.shape[1] // 2], 1)\n",
    "        hx = torch.tanh(hx)\n",
    "        return x, hx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149660fa-baf9-4994-b351-dab1639d000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_f shape: torch.Size([2, 128, 64, 64])\n",
      "h_f shape: torch.Size([2, 128, 64, 64])\n",
      "x shape prior to norm: torch.Size([2, 128, 64, 64])\n",
      "x_f shape: torch.Size([2, 128, 32, 32])\n",
      "h_f shape: torch.Size([2, 128, 32, 32])\n",
      "x shape prior to norm: torch.Size([2, 128, 32, 32])\n",
      "x_f shape: torch.Size([2, 128, 16, 16])\n",
      "h_f shape: torch.Size([2, 128, 16, 16])\n",
      "x shape prior to norm: torch.Size([2, 128, 16, 16])\n",
      "len x_forwards: 3\n",
      "x shape prior to norm: torch.Size([2, 128, 8, 8])\n",
      "input_x before interpolate: torch.Size([2, 3, 256, 256])\n",
      "input_x after interpolate: torch.Size([2, 3, 32, 32])\n",
      "x shape prior to norm: torch.Size([2, 128, 16, 16])\n",
      "input_x before interpolate: torch.Size([2, 3, 256, 256])\n",
      "input_x after interpolate: torch.Size([2, 3, 64, 64])\n",
      "x shape prior to norm: torch.Size([2, 128, 32, 32])\n",
      "input_x before interpolate: torch.Size([2, 3, 256, 256])\n",
      "input_x after interpolate: torch.Size([2, 3, 128, 128])\n",
      "x shape prior to norm: torch.Size([2, 128, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 128 elements not 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m BidirEncoder()\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_img1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_img2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_pyr_levels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 70\u001b[0m, in \u001b[0;36mBidirEncoder.forward\u001b[0;34m(self, x, y, pyr_levels)\u001b[0m\n\u001b[1;32m     67\u001b[0m x0, hx0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_init_stat(x)\n\u001b[1;32m     68\u001b[0m y0, hy0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_init_stat(y)\n\u001b[0;32m---> 70\u001b[0m x_pyramid, y_pyramid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhy0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpyr_levels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_pyramid[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], y_pyramid[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 157\u001b[0m, in \u001b[0;36mBidirEncoder._encode\u001b[0;34m(self, x0, hx0, y0, hy0, pyr_levels, input_x, input_y)\u001b[0m\n\u001b[1;32m    155\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_merge_conv(F\u001b[38;5;241m.\u001b[39mrelu(x))\n\u001b[1;32m    156\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_merge_conv(F\u001b[38;5;241m.\u001b[39mrelu(y))\n\u001b[0;32m--> 157\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_cgu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_1x1_abs_chs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_1x1_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/private/final_project/DPFlow/architecture_files/cross_gated_unit.py:198\u001b[0m, in \u001b[0;36mCrossGateBlock.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Normalize data prior to CGU forward pass\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx shape prior to norm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(y)\n\u001b[1;32m    201\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_gate_left(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2482\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2480\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 128 elements not 1"
     ]
    }
   ],
   "source": [
    "example_img1 = torch.rand((2, 3, 256, 256), device=device)\n",
    "example_img2 = torch.rand((2, 3, 256, 256), device=device)\n",
    "example_pyr_levels = 3\n",
    "\n",
    "model = BidirEncoder()\n",
    "model.to(device)\n",
    "output = model(example_img1, example_img2, example_pyr_levels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
